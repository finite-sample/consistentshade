{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0891f187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results over 6 replicates ===\n",
      "Baseline  : RMSE = 24.48 ± 0.21   Stability = 4.52\n",
      "k=3, λ=.05: RMSE = 26.07 ± 0.40   Stability = 3.70\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "Micro-Bootstrap Ensemble (K=3) vs. Baseline MLP\n",
    "============================================================\n",
    "* Synthetic regression, 20 features\n",
    "* Baseline: single Dropout MLP\n",
    "* k3_boot: three shadow copies, per-batch bootstrap + variance penalty\n",
    "* Metrics:  OOS RMSE  |  prediction-stability RMSE (√mean var across runs)\n",
    "============================================================\n",
    "\"\"\"\n",
    "import copy, itertools, math, random, numpy as np, torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0.  Helpers\n",
    "# -------------------------------------------------------------------\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "def pred_stability(pred_matrix: np.ndarray) -> float:\n",
    "    \"\"\"RMSE of predictions across bootstrap fits: √(mean var_per_sample).\"\"\"\n",
    "    return math.sqrt(pred_matrix.var(axis=0).mean())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1.  Synthetic data\n",
    "# -------------------------------------------------------------------\n",
    "set_seed(0)\n",
    "X, y = make_regression(\n",
    "    n_samples=3500, n_features=20, n_informative=15, noise=20.0, random_state=0\n",
    ")\n",
    "X = StandardScaler().fit_transform(X).astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1\n",
    ")\n",
    "train_ds = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr))\n",
    "TEST_X  = torch.tensor(X_te)\n",
    "TEST_Y  = torch.tensor(y_te)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2.  Model\n",
    "# -------------------------------------------------------------------\n",
    "class DropMLP(nn.Module):\n",
    "    def __init__(self, d_in=20, hid=64, p=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, 1)\n",
    "        )\n",
    "    def forward(self, x):  # → (B,)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3.  Training routines\n",
    "# -------------------------------------------------------------------\n",
    "def train_baseline(seed, *, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    model = DropMLP()\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_ds, batch_size=bs, sampler=RandomSampler(train_ds))\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.mse_loss(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # MC-Dropout average for evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.stack([model(TEST_X) for _ in range(4)]).mean(0)\n",
    "        rmse  = torch.sqrt(F.mse_loss(preds, TEST_Y)).item()\n",
    "    return preds.numpy(), rmse\n",
    "\n",
    "\n",
    "def train_k3(seed, *, K=3, lam=0.05, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    models = nn.ModuleList([DropMLP() for _ in range(K)])\n",
    "    opt = torch.optim.Adam(\n",
    "        itertools.chain(*[m.parameters() for m in models]), lr=lr\n",
    "    )\n",
    "    loader = DataLoader(train_ds, batch_size=bs, sampler=RandomSampler(train_ds))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            preds, sup_losses = [], []\n",
    "            bootstrap_indices = []\n",
    "\n",
    "            # ----- forward K bootstrap shadows -----\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                bootstrap_indices.append(idx)\n",
    "                px = m(xb[idx])          # predictions\n",
    "                preds.append(px)\n",
    "                sup_losses.append(F.mse_loss(px, yb[idx]))\n",
    "\n",
    "            preds     = torch.stack(preds)                 # (K,B)\n",
    "            sup_loss  = torch.stack(sup_losses).mean()\n",
    "            var_pen   = preds.var(dim=0, unbiased=False).mean()\n",
    "            loss      = sup_loss + lam * var_pen\n",
    "\n",
    "            # ----- update -----\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # ----- evaluation (ensemble average) -----\n",
    "    for m in models: m.eval()\n",
    "    with torch.no_grad():\n",
    "        ens_preds = torch.stack([m(TEST_X) for m in models]).mean(0)\n",
    "        rmse      = torch.sqrt(F.mse_loss(ens_preds, TEST_Y)).item()\n",
    "    return ens_preds.numpy(), rmse\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4.  Experiment: 6 fits each\n",
    "# -------------------------------------------------------------------\n",
    "R = 6                  # number of independent training replicates\n",
    "baseline_preds, k3_preds = [], []\n",
    "baseline_rmse,  k3_rmse  = [], []\n",
    "\n",
    "for r in range(R):\n",
    "    seed = 20250712 + r\n",
    "    p0, e0 = train_baseline(seed)\n",
    "    p1, e1 = train_k3(seed, K=3, lam=0.05)\n",
    "    baseline_preds.append(p0); baseline_rmse.append(e0)\n",
    "    k3_preds.append(p1);       k3_rmse.append(e1)\n",
    "\n",
    "baseline_preds = np.stack(baseline_preds)\n",
    "k3_preds       = np.stack(k3_preds)\n",
    "\n",
    "print(\"=== Results over\", R, \"replicates ===\")\n",
    "print(f\"Baseline  : RMSE = {np.mean(baseline_rmse):.2f} ± {np.std(baseline_rmse):.2f}   \"\n",
    "      f\"Stability = {pred_stability(baseline_preds):.2f}\")\n",
    "print(f\"k=3, λ=.05: RMSE = {np.mean(k3_rmse):.2f} ± {np.std(k3_rmse):.2f}   \"\n",
    "      f\"Stability = {pred_stability(k3_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30471cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>avg_test_RMSE</th>\n",
       "      <th>std_test_RMSE</th>\n",
       "      <th>prediction_stability_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>24.003791</td>\n",
       "      <td>0.391080</td>\n",
       "      <td>5.367281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k=3</td>\n",
       "      <td>27.379538</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>3.457137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method  avg_test_RMSE  std_test_RMSE  prediction_stability_RMSE\n",
       "0  baseline      24.003791       0.391080                   5.367281\n",
       "1       k=3      27.379538       0.501202                   3.457137"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "import numpy as np, random, math, itertools, pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def stability(pred_matrix: np.ndarray) -> float:\n",
    "    \"\"\"RMSE of predictions across fits.\"\"\"\n",
    "    return math.sqrt(pred_matrix.var(axis=0).mean())\n",
    "\n",
    "# ----------------- Data -----------------\n",
    "X, y = make_regression(\n",
    "    n_samples=4000, n_features=20, n_informative=15, noise=20.0, random_state=0\n",
    ")\n",
    "X = StandardScaler().fit_transform(X).astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "train_ds = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr))\n",
    "TEST_X = torch.tensor(X_te)\n",
    "TEST_Y = torch.tensor(y_te)\n",
    "\n",
    "# ----------------- Model -----------------\n",
    "class DropMLP(nn.Module):\n",
    "    def __init__(self, d_in=20, hid=64, p=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "# ----------------- Training routines -----------------\n",
    "def train_baseline(seed, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    model = DropMLP()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_ds, batch_size=bs, sampler=RandomSampler(train_ds))\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.mse_loss(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.stack([model(TEST_X) for _ in range(4)]).mean(0)\n",
    "        rmse = torch.sqrt(F.mse_loss(preds, TEST_Y)).item()\n",
    "    return preds.numpy(), rmse\n",
    "\n",
    "def train_k3(seed, lam=0.05, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    K = 3\n",
    "    models = nn.ModuleList([DropMLP() for _ in range(K)])\n",
    "    opt = torch.optim.Adam(itertools.chain(*[m.parameters() for m in models]), lr=lr)\n",
    "    loader = DataLoader(train_ds, batch_size=bs, sampler=RandomSampler(train_ds))\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            preds = []\n",
    "            sup_losses = []\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                pred = m(xb[idx])\n",
    "                preds.append(pred)\n",
    "                sup_losses.append(F.mse_loss(pred, yb[idx]))\n",
    "            preds = torch.stack(preds)  # K x B\n",
    "            sup_loss = torch.stack(sup_losses).mean()\n",
    "            var_pen = preds.var(dim=0, unbiased=False).mean()\n",
    "            loss = sup_loss + lam * var_pen\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    with torch.no_grad():\n",
    "        ensemble_preds = torch.stack([m(TEST_X) for m in models]).mean(0)\n",
    "        rmse = torch.sqrt(F.mse_loss(ensemble_preds, TEST_Y)).item()\n",
    "    return ensemble_preds.numpy(), rmse\n",
    "\n",
    "# ----------------- Comprehensive Evaluation -----------------\n",
    "R = 30  # number of independent replicates\n",
    "baseline_preds_list, k3_preds_list = [], []\n",
    "baseline_rmse_list, k3_rmse_list = [], []\n",
    "\n",
    "for r in range(R):\n",
    "    seed = 12345 + r\n",
    "    bpred, brmse = train_baseline(seed)\n",
    "    kpred, krmse = train_k3(seed)\n",
    "    baseline_preds_list.append(bpred); baseline_rmse_list.append(brmse)\n",
    "    k3_preds_list.append(kpred);        k3_rmse_list.append(krmse)\n",
    "\n",
    "baseline_preds = np.stack(baseline_preds_list)\n",
    "k3_preds = np.stack(k3_preds_list)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"method\": [\"baseline\", \"k=3\"],\n",
    "    \"avg_test_RMSE\": [np.mean(baseline_rmse_list), np.mean(k3_rmse_list)],\n",
    "    \"std_test_RMSE\": [np.std(baseline_rmse_list), np.std(k3_rmse_list)],\n",
    "    \"prediction_stability_RMSE\": [stability(baseline_preds), stability(k3_preds)]\n",
    "})\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24f6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Synthetic regression (30 runs) ===\n",
      "     method  avg_RMSE  std_RMSE  prediction_stability_RMSE\n",
      "0  baseline    23.878     0.371                      5.293\n",
      "1       k=3    29.457     0.642                      3.275\n",
      "\n",
      "=== Adult income (30 runs) ===\n",
      "     method  avg_Accuracy  std_Accuracy  logit_stability_RMSE\n",
      "0  baseline         0.826         0.001                 0.236\n",
      "1       k=3         0.825         0.001                 0.044\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -------------------------------------------------------------\n",
    "# Bootstrap‑variance experiment: synthetic regression + Adult income\n",
    "# -------------------------------------------------------------\n",
    "import math, random, itertools, inspect, warnings\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.datasets import make_regression, fetch_openml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from packaging import version\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "def stability(pred_mat: np.ndarray) -> float:\n",
    "    \"\"\"sqrt(mean variance) across fits (rows) at each sample (cols)\"\"\"\n",
    "    return math.sqrt(pred_mat.var(axis=0).mean())\n",
    "\n",
    "# ---------------- synthetic regression data ----------------\n",
    "set_seed(0)\n",
    "X_syn, y_syn = make_regression(\n",
    "    n_samples=4000, n_features=20, n_informative=15, noise=20.0, random_state=0\n",
    ")\n",
    "X_syn = StandardScaler().fit_transform(X_syn).astype(\"float32\")\n",
    "y_syn = y_syn.astype(\"float32\")\n",
    "Xtr_syn, Xte_syn, ytr_syn, yte_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.25, random_state=1\n",
    ")\n",
    "train_syn = TensorDataset(torch.tensor(Xtr_syn), torch.tensor(ytr_syn))\n",
    "TEST_X_syn = torch.tensor(Xte_syn)\n",
    "TEST_Y_syn = torch.tensor(yte_syn)\n",
    "\n",
    "# ---------------- Adult income data ----------------\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "X_df = adult.data\n",
    "y_ser = (adult.target == \">50K\").astype(\"int64\")          # 0/1 label\n",
    "\n",
    "num_cols = X_df.select_dtypes(\"number\").columns.to_list()\n",
    "cat_cols = X_df.select_dtypes(\"object\").columns.to_list()\n",
    "\n",
    "# robust OneHotEncoder\n",
    "if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", enc,            cat_cols)\n",
    "])\n",
    "X_adult = preproc.fit_transform(X_df).astype(\"float32\")\n",
    "y_adult = y_ser.to_numpy()\n",
    "\n",
    "Xtr_A, Xte_A, ytr_A, yte_A = train_test_split(\n",
    "    X_adult, y_adult, test_size=0.25, random_state=1, stratify=y_adult\n",
    ")\n",
    "train_A = TensorDataset(torch.tensor(Xtr_A), torch.tensor(ytr_A))\n",
    "TEST_X_A = torch.tensor(Xte_A)\n",
    "TEST_Y_A = torch.tensor(yte_A)\n",
    "\n",
    "# ---------------- model defs ----------------\n",
    "class DropMLP(nn.Module):\n",
    "    def __init__(self, d_in, hid=64, p=0.2, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, hid), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out.squeeze(-1) if out.shape[1] == 1 else out\n",
    "\n",
    "# ---------------- training functions ----------------\n",
    "def train_syn_baseline(seed, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    model = DropMLP(d_in=X_syn.shape[1], out_dim=1)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_syn, batch_size=bs, sampler=RandomSampler(train_syn))\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.mse_loss(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.stack([model(TEST_X_syn) for _ in range(4)]).mean(0)\n",
    "        rmse  = torch.sqrt(F.mse_loss(preds, TEST_Y_syn)).item()\n",
    "    return preds.numpy(), rmse\n",
    "\n",
    "def train_syn_k3(seed, lam=0.05, epochs=25, bs=64, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    K = 3\n",
    "    models = nn.ModuleList([DropMLP(d_in=X_syn.shape[1], out_dim=1) for _ in range(K)])\n",
    "    opt = torch.optim.Adam(itertools.chain(*(m.parameters() for m in models)), lr=lr)\n",
    "    loader = DataLoader(train_syn, batch_size=bs, sampler=RandomSampler(train_syn))\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            preds, sup = [], []\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                p   = m(xb[idx])\n",
    "                preds.append(p)\n",
    "                sup.append(F.mse_loss(p, yb[idx]))\n",
    "            preds = torch.stack(preds)              # (K,B)\n",
    "            loss  = torch.stack(sup).mean() + lam * preds.var(0).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    for m in models: m.eval()\n",
    "    with torch.no_grad():\n",
    "        ens = torch.stack([m(TEST_X_syn) for m in models]).mean(0)\n",
    "        rmse = torch.sqrt(F.mse_loss(ens, TEST_Y_syn)).item()\n",
    "    return ens.numpy(), rmse\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def train_A_baseline(seed, epochs=20, bs=256, lr=3e-4):\n",
    "    set_seed(seed)\n",
    "    d_in = X_adult.shape[1]\n",
    "    model = DropMLP(d_in=d_in, hid=128, p=0.3, out_dim=2)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_A, batch_size=bs, sampler=RandomSampler(train_A))\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.cross_entropy(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = torch.stack([model(TEST_X_A) for _ in range(4)]).mean(0)\n",
    "        acc    = accuracy(logits, TEST_Y_A)\n",
    "    return logits.numpy(), acc\n",
    "\n",
    "def train_A_k3(seed, lam=0.05, epochs=20, bs=256, lr=3e-4):\n",
    "    set_seed(seed)\n",
    "    d_in = X_adult.shape[1]\n",
    "    K = 3\n",
    "    models = nn.ModuleList([DropMLP(d_in=d_in, hid=128, p=0.3, out_dim=2) for _ in range(K)])\n",
    "    opt = torch.optim.Adam(itertools.chain(*(m.parameters() for m in models)), lr=lr)\n",
    "    loader = DataLoader(train_A, batch_size=bs, sampler=RandomSampler(train_A))\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            logitsK, sup = [], []\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                out = m(xb[idx])\n",
    "                logitsK.append(out)\n",
    "                sup.append(F.cross_entropy(out, yb[idx]))\n",
    "            logitsK = torch.stack(logitsK)          # (K,B,2)\n",
    "            loss = torch.stack(sup).mean() + lam * logitsK.var(0).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    for m in models: m.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = torch.stack([m(TEST_X_A) for m in models]).mean(0)\n",
    "        acc    = accuracy(logits, TEST_Y_A)\n",
    "    return logits.numpy(), acc\n",
    "\n",
    "# ---------------- run 30 replicates ----------------\n",
    "R = 30\n",
    "syn_base_preds, syn_k3_preds = [], []\n",
    "syn_base_rmse, syn_k3_rmse   = [], []\n",
    "A_base_logits, A_k3_logits   = [], []\n",
    "A_base_acc,   A_k3_acc       = [], []\n",
    "\n",
    "for r in range(R):\n",
    "    seed = 1234 + r\n",
    "    bp, br = train_syn_baseline(seed)\n",
    "    kp, kr = train_syn_k3(seed)\n",
    "    syn_base_preds.append(bp); syn_base_rmse.append(br)\n",
    "    syn_k3_preds.append(kp);   syn_k3_rmse.append(kr)\n",
    "\n",
    "    bl, ba = train_A_baseline(seed)\n",
    "    kl, ka = train_A_k3(seed)\n",
    "    A_base_logits.append(bl); A_base_acc.append(ba)\n",
    "    A_k3_logits.append(kl);   A_k3_acc.append(ka)\n",
    "\n",
    "# ---------------- print summaries ----------------\n",
    "syn_summary = pd.DataFrame({\n",
    "    \"method\": [\"baseline\", \"k=3\"],\n",
    "    \"avg_RMSE\": [np.mean(syn_base_rmse), np.mean(syn_k3_rmse)],\n",
    "    \"std_RMSE\": [np.std(syn_base_rmse), np.std(syn_k3_rmse)],\n",
    "    \"prediction_stability_RMSE\": [\n",
    "        stability(np.stack(syn_base_preds)),\n",
    "        stability(np.stack(syn_k3_preds))\n",
    "    ]\n",
    "})\n",
    "\n",
    "adult_summary = pd.DataFrame({\n",
    "    \"method\": [\"baseline\", \"k=3\"],\n",
    "    \"avg_Accuracy\": [np.mean(A_base_acc), np.mean(A_k3_acc)],\n",
    "    \"std_Accuracy\": [np.std(A_base_acc), np.std(A_k3_acc)],\n",
    "    \"logit_stability_RMSE\": [\n",
    "        stability(np.stack(A_base_logits)),\n",
    "        stability(np.stack(A_k3_logits))\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Synthetic regression (30 runs) ===\")\n",
    "print(syn_summary.round(3))\n",
    "print(\"\\n=== Adult income (30 runs) ===\")\n",
    "print(adult_summary.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4bf7d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soodoku/opt/anaconda3/envs/py311ds/lib/python3.11/site-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1. Available versions:\n",
      "- version 1, status: active\n",
      "  url: https://www.openml.org/search?type=data&id=31\n",
      "- version 2, status: active\n",
      "  url: https://www.openml.org/search?type=data&id=44096\n",
      "\n",
      "  warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# California Housing (regression)\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cal = fetch_california_housing(as_frame=True)\n",
    "X_cal = cal.data.values.astype(\"float32\")\n",
    "y_cal = cal.target.values.astype(\"float32\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_cal = StandardScaler().fit_transform(X_cal).astype(\"float32\")\n",
    "\n",
    "Xtr_C, Xte_C, ytr_C, yte_C = train_test_split(\n",
    "    X_cal, y_cal, test_size=0.25, random_state=1\n",
    ")\n",
    "train_C = TensorDataset(torch.tensor(Xtr_C), torch.tensor(ytr_C))\n",
    "TEST_X_C = torch.tensor(Xte_C)\n",
    "TEST_Y_C = torch.tensor(yte_C)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Credit Card Default (binary classification)\n",
    "# ------------------------------------------------------------\n",
    "credit = fetch_openml(\"credit-g\", version=1, as_frame=True)     # German credit\n",
    "X_cr = credit.data\n",
    "y_cr = (credit.target == \"good\").astype(\"int64\")      # 1 = good credit\n",
    "\n",
    "num_cols_cr = X_cr.select_dtypes(\"number\").columns.to_list()\n",
    "cat_cols_cr = X_cr.select_dtypes(\"object\").columns.to_list()\n",
    "\n",
    "# robust OneHotEncoder (same trick as before)\n",
    "if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "    enc_cr = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    enc_cr = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "pre_cr = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols_cr),\n",
    "    (\"cat\", enc_cr,          cat_cols_cr)\n",
    "])\n",
    "X_cr = pre_cr.fit_transform(X_cr).astype(\"float32\")\n",
    "y_cr = y_cr.to_numpy()\n",
    "\n",
    "Xtr_CR, Xte_CR, ytr_CR, yte_CR = train_test_split(\n",
    "    X_cr, y_cr, test_size=0.25, random_state=1, stratify=y_cr\n",
    ")\n",
    "train_CR = TensorDataset(torch.tensor(Xtr_CR), torch.tensor(ytr_CR))\n",
    "TEST_X_CR = torch.tensor(Xte_CR)\n",
    "TEST_Y_CR = torch.tensor(yte_CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a479c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- California Housing training ----------------\n",
    "def train_C_baseline(seed, epochs=20, bs=128, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    model = DropMLP(d_in=X_cal.shape[1], hid=64, p=0.2, out_dim=1)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_C, batch_size=bs, sampler=RandomSampler(train_C))\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.mse_loss(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.stack([model(TEST_X_C) for _ in range(4)]).mean(0)\n",
    "        rmse  = torch.sqrt(F.mse_loss(preds, TEST_Y_C)).item()\n",
    "    return preds.numpy(), rmse\n",
    "\n",
    "def train_C_k3(seed, lam=0.05, epochs=20, bs=128, lr=1e-3):\n",
    "    set_seed(seed)\n",
    "    K = 3\n",
    "    models = nn.ModuleList([DropMLP(d_in=X_cal.shape[1], hid=64, p=0.2, out_dim=1) for _ in range(K)])\n",
    "    opt = torch.optim.Adam(itertools.chain(*(m.parameters() for m in models)), lr=lr)\n",
    "    loader = DataLoader(train_C, batch_size=bs, sampler=RandomSampler(train_C))\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            preds, sup = [], []\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                p = m(xb[idx]); preds.append(p)\n",
    "                sup.append(F.mse_loss(p, yb[idx]))\n",
    "            preds = torch.stack(preds)\n",
    "            loss  = torch.stack(sup).mean() + lam * preds.var(0).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    for m in models: m.eval()\n",
    "    with torch.no_grad():\n",
    "        ens = torch.stack([m(TEST_X_C) for m in models]).mean(0)\n",
    "        rmse = torch.sqrt(F.mse_loss(ens, TEST_Y_C)).item()\n",
    "    return ens.numpy(), rmse\n",
    "\n",
    "# ---------------- Credit Default training ----------------\n",
    "def train_CR_baseline(seed, epochs=20, bs=256, lr=3e-4):\n",
    "    set_seed(seed)\n",
    "    d_in = X_cr.shape[1]\n",
    "    model = DropMLP(d_in=d_in, hid=128, p=0.3, out_dim=2)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(train_CR, batch_size=bs, sampler=RandomSampler(train_CR))\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            loss = F.cross_entropy(model(xb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = torch.stack([model(TEST_X_CR) for _ in range(4)]).mean(0)\n",
    "        acc    = accuracy(logits, TEST_Y_CR)\n",
    "    return logits.numpy(), acc\n",
    "\n",
    "def train_CR_k3(seed, lam=0.05, epochs=20, bs=256, lr=3e-4):\n",
    "    set_seed(seed)\n",
    "    d_in = X_cr.shape[1]; K = 3\n",
    "    models = nn.ModuleList([DropMLP(d_in=d_in, hid=128, p=0.3, out_dim=2) for _ in range(K)])\n",
    "    opt = torch.optim.Adam(itertools.chain(*(m.parameters() for m in models)), lr=lr)\n",
    "    loader = DataLoader(train_CR, batch_size=bs, sampler=RandomSampler(train_CR))\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            logitsK, sup = [], []\n",
    "            for m in models:\n",
    "                idx = torch.randint(0, len(xb), (len(xb),))\n",
    "                out = m(xb[idx])\n",
    "                logitsK.append(out)\n",
    "                sup.append(F.cross_entropy(out, yb[idx]))\n",
    "            logitsK = torch.stack(logitsK)\n",
    "            loss = torch.stack(sup).mean() + lam * logitsK.var(0).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    for m in models: m.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = torch.stack([m(TEST_X_CR) for m in models]).mean(0)\n",
    "        acc    = accuracy(logits, TEST_Y_CR)\n",
    "    return logits.numpy(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd77f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== California Housing (30 runs) ===\n",
      "     method  avg_RMSE  std_RMSE  prediction_stability_RMSE\n",
      "0  baseline     0.591     0.005                      0.074\n",
      "1       k=3     0.598     0.004                      0.054\n",
      "\n",
      "=== Credit Default (30 runs) ===\n",
      "     method  avg_Accuracy  std_Accuracy  logit_stability_RMSE\n",
      "0  baseline         0.697         0.009                 0.118\n",
      "1       k=3         0.688         0.006                 0.061\n"
     ]
    }
   ],
   "source": [
    "# ---- arrays to collect\n",
    "cal_base_preds, cal_k3_preds = [], []\n",
    "cal_base_rmse, cal_k3_rmse   = [], []\n",
    "CR_base_logits, CR_k3_logits = [], []\n",
    "CR_base_acc,   CR_k3_acc     = [], []\n",
    "\n",
    "for r in range(R):\n",
    "    seed = 1234 + r\n",
    "    # existing synthetic + adult code ...\n",
    "    # ---- California\n",
    "    bp, br = train_C_baseline(seed)\n",
    "    kp, kr = train_C_k3(seed)\n",
    "    cal_base_preds.append(bp); cal_base_rmse.append(br)\n",
    "    cal_k3_preds.append(kp);   cal_k3_rmse.append(kr)\n",
    "    # ---- Credit Default\n",
    "    bl, ba = train_CR_baseline(seed)\n",
    "    kl, ka = train_CR_k3(seed)\n",
    "    CR_base_logits.append(bl); CR_base_acc.append(ba)\n",
    "    CR_k3_logits.append(kl);   CR_k3_acc.append(ka)\n",
    "\n",
    "# ---- summaries\n",
    "cal_summary = pd.DataFrame({\n",
    "    \"method\": [\"baseline\",\"k=3\"],\n",
    "    \"avg_RMSE\":[np.mean(cal_base_rmse), np.mean(cal_k3_rmse)],\n",
    "    \"std_RMSE\":[np.std(cal_base_rmse), np.std(cal_k3_rmse)],\n",
    "    \"prediction_stability_RMSE\":[stability(np.stack(cal_base_preds)),\n",
    "                                 stability(np.stack(cal_k3_preds))]\n",
    "})\n",
    "\n",
    "credit_summary = pd.DataFrame({\n",
    "    \"method\": [\"baseline\",\"k=3\"],\n",
    "    \"avg_Accuracy\":[np.mean(CR_base_acc), np.mean(CR_k3_acc)],\n",
    "    \"std_Accuracy\":[np.std(CR_base_acc), np.std(CR_k3_acc)],\n",
    "    \"logit_stability_RMSE\":[stability(np.stack(CR_base_logits)),\n",
    "                            stability(np.stack(CR_k3_logits))]\n",
    "})\n",
    "\n",
    "print(\"\\n=== California Housing (30 runs) ===\")\n",
    "print(cal_summary.round(3))\n",
    "print(\"\\n=== Credit Default (30 runs) ===\")\n",
    "print(credit_summary.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65114289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
